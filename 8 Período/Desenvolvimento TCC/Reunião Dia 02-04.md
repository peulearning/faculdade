
Executar os Steps e Explicar [ X ] 
Entender porque a dimens√£o 224x224 foi utilizada [ X ]  ( Poss√≠vel Testar em resolu√ß√£o 256x256 ou 512x512 )  
Procurar Artigos 5 √° 10 quais motiva√ß√µes de utilizar Vis√£o Computacional com Feridas. [ x ]  
### üìå **1. Padr√£o de Modelos Pr√©-Treinados**

A resolu√ß√£o **224√ó224** foi popularizada pelo modelo **AlexNet** (2012) e depois utilizada em redes como **VGG16, ResNet, MobileNet e EfficientNet**. Muitas arquiteturas modernas usam esse tamanho como entrada padr√£o, pois os pesos pr√©-treinados esperam essa dimens√£o.

**Exemplo:** Modelos como `ResNet-50` e `MobileNetV2` aceitam 224x224 como entrada padr√£o.
 
### üìå **2. Efici√™ncia Computacional e Mem√≥ria**

Redes neurais convolucionais (CNNs) operam melhor quando a imagem tem **tamanho fixo**. Se as imagens fossem maiores, exigiriam mais mem√≥ria e processamento. Se fossem muito pequenas, perderiam detalhes importantes.

- **224x224** √© um **compromisso entre qualidade e efici√™ncia**.
    
- **Menores que isso**: perda de detalhes.
    
- **Maiores que isso**: aumento do custo computacional.
    

**Exemplo pr√°tico:**  
Uma imagem de **1024x1024** tem **1.048.576 pixels**. J√° uma de **224x224** tem **50.176 pixels**, sendo **20√ó menor**, reduzindo o tempo de treinamento.

### **üìå 3. Preserva√ß√£o de Informa√ß√µes Relevantes**

Ao redimensionar para **224x224**, **mantemos detalhes essenciais** da imagem sem comprometer a qualidade para redes neurais convolucionais (CNNs).  
Se usarmos um tamanho muito pequeno, a CNN pode perder informa√ß√µes importantes, especialmente para **segmenta√ß√£o e classifica√ß√£o de les√µes cut√¢neas**.

**Exemplo:**  
Uma les√£o pequena pode desaparecer se reduzirmos para **64x64**, mas em **224x224** ainda conseguimos ver bordas e texturas.

### üìå **4. Aplica√ß√£o no Reconhecimento de Les√µes Cut√¢neas**

Em aplica√ß√µes m√©dicas, como a **segmenta√ß√£o e classifica√ß√£o de feridas**, a dimens√£o **224x224** √© suficiente para capturar caracter√≠sticas como:

- **Textura da pele**
    
- **Bordas da les√£o**
    
- **Padr√µes de colora√ß√£o**
    
- **Detalhes pequenos, mas importantes**
    

Se aumentarmos muito a resolu√ß√£o, apenas aumentamos o custo de processamento **sem melhorar significativamente a precis√£o**.

### üìå **5. Consist√™ncia entre Amostras**

Usar **224x224** para todas as imagens padroniza os dados de entrada, tornando o treinamento mais eficiente e evitando que a CNN precise lidar com tamanhos variados.

Sem um tamanho fixo, precisar√≠amos de **camadas adaptativas** ou realizar **padding**, o que poderia introduzir distor√ß√µes.


### ‚ö° **1. Redes  Convolucionais Par√¢metros T√©cnicos**

https://www.youtube.com/watch?v=lb5ocJiDLgg&t=1268s - Redes Neurais Convolucionais

Estamos Tratando de Dados Complexos ( Imagens )

* Imagem , Texto, Grafos, A√∫dio.

Dados Complexos : Alta dimensionalidade (Grande n√∫mero de Atributos e Caracter√≠sticas ) em outras palavras cada pixel e um dado.  Uma imagem de 7x7 pixels e menos que um √≠cone ou seja mesmo pequenas imagem pequenas existe uma quantidade absurda de atributos. ainda assim os dados s√£o " N√ÉO ESTRUTURADOS " pois as informa√ß√µes necess√°rias est√£o distribu√≠das em v√°rios pixels, dando o fato que precisa ser LIMPADO para conseguirmos extrair as caracter√≠sticas necess√°rias.

Problemas dos dados  Complexos : Redund√¢ncia doa dados, padr√µes distribu√≠dos em muitos atributos .  Feature Engineering  ( Limpeza de Dados, Transforma√ß√£o dos Dados, Sele√ß√£o das Caracter√≠sticas ), Reconhecimento de Padr√µes . 

Exemplo  : Conjunto de imagem de D√≠gitos  cujo objetivo e identificar dentro das imagens os subconjuntos de pixels (atributos caracter√≠sticas ) que identifique um determinado padr√£o  ent√£o o algoritmo deve encontrar as caracter√≠sticas mais fortes de uma classe para outras e ap√≥s a extra√ß√£o desse conjunto de pixels s√£o as caracter√≠sticas e ap√≥s extra√≠dos 

Aprendizagem de M√°quina Cl√°ssico ( Shallow ) era utilizado essa etapas at√© agora feito a m√£o

* Atributos Simples, Estruturados, Baixa Dimensionalidade  
*  Previamente Selecionados / Manipulados ( Engenharia de Caracter√≠sticas )
* Modelos Simples ( Rasos - Shallow ( OpenCV2  >> SVM  ( Classificador Linear Filtros  )))
* Poucos par√¢metros

Aprendizado Profundo ( Deep Learning ) em outra palavras o modelo aprende sozinho apenas com a imagem crua , sem que eu tenha que pr√©-processar os dados.

* Atributos Complexos :  N√£o estruturados, Alta Dimensionalidade
* Previamente Selecionados / Manipulados ( Engenharia de Caracter√≠sticas )
*  O pr√≥prio modelo aprende como : Selecionar e extrair caracter√≠sticas , reconhece padr√µes de caracter√≠sticas
* Modelos Complexos : Milhares  / Milh√µes de Par√¢metros 

Redes Neurais Convolucionais :  S√£o redes neurais que trabalham com qualquer tipo de dado por√©m vamos fixar com Imagens .

* Convolu√ß√µes de Dimens√£o 1 :  Para vetores
* Convolu√ß√µes de Dimens√£o 2 : Matrizes =  Imagens
* Convolu√ß√µes de Dimens√£o 3 : Matrizes de Matrizes = Volumes

Em outras palavras n√£o estamos criando uma outra rede neural , apenas estamos adicionando camadas novas dando suas respectivas responsabilidades.

*  Bloco 1  : Redu√ß√£o de Dimensionalidade , Extra√ß√£o de Caracter√≠sticas  ( Camadas de Convolu√ß√µes , por exemplo Pooling, Conv, ReLU )
*  Bloco 2 :  Reconhecimento de Classifica√ß√£o de Padr√µes

Tipos de Camadas : 

* Convolucionais
* Dimensionalidade (Pooling / Up Sampling )
* Densas / Totalmente Conectadas (MLP)
* Normaliza√ß√£o (Dropout , Normalization, Nose )

Fun√ß√µes de Ativa√ß√£o ( Para √∫ltima camada da Rede ) :

*  Fam√≠lia ReLU 
*  Fam√≠lia Softmax ( Cara Computacionalmente ) : Quanto mais sa√≠das eu tiver mais caro ser√° ( O(n ))

 
	Em outras palavras as CNN's elas ter√£o muitas camadas de convul√ß√£o, pooling, relu e perceptrons, com isso temos a primeira parte ou seja o Pass Forward e geralmente o output e a Classifica√ß√£o ou seja  essa sa√≠da e uma distribui√ß√£o de probabilidade que ser√° associada a cada classe poss√≠vel . O modelo ent√£o e treinado comparando a distribui√ß√£o de probabilidades com a classe verdadeira , sendo realizada pela fun√ß√£o de custo ( otimizador ) .


Da pen√∫ltima camada para tr√°s a gente usa a ReLU  seria o estado da arte √© que funciona bem para todos os modelos, e a √∫ltima camada e utilizado o Softmax ( problemas de classifica√ß√£o ) .


Otimizadores / M√©todos de Treinamento : 


Gradiente Descendente [ Batch / Mini-Batch / Stochastic ]  ( Pelo menos e oque estamos utilizando) e um Suavizador  dos pesos do modelo,  mas em outras palavras cada otimizados da um modelo diferente. e por padr√£o utiliza-se o descendente. 


* Fun√ß√µes de Custo : 

Entropia Cruzada  e uma fun√ß√£o de custo que tem que ser utilizada quando a fun√ß√£o de ativa√ß√£o da √∫ltima camada for uma sigm√≥ide ou uma softmax .  E isso porque ela compara o qu√£o parecidas s√£o duas distribui√ß√µes de probabilidade da sua rede.  

No caso utilizamos a Entropia Cruzada  Bin√°ria e ( 0, 1 )  e deveremos utilizar a Entropia Cruzada Multinomial.

Arquitetura do Modelo

Componentes da Camada Convolucional 

* Quantos Filtros ? 
* Qual a dimens√£o de cada Fitlro ?
* Qual o incremento entre os filtros (stride) ? 
* Qual e a fun√ß√£o de ativa√ß√£o ?  N√£o e obrigat√≥rio por√©m podem passar valores negativos.

Camadas de Pooling  ( Redu√ß√£o , Amostra , Pixels Importante )

* Qual tipo do pooling ? ( M√°ximo , m√©dia, m√≠nimo)
* Qual a dimens√£o do filtro ? 2x2 , 3x3 , 4x4  .... 
* Quanto maior a dimens√£o maior a redu√ß√£o da imagem e quanto maior a redu√ß√£o da imagem mais perde defini√ß√£o.

Camadas de Perceptrons ( Densas / Totalmente Conectadas )

* Quantas camadas ? 
* Quantos neur√¥nios por camda ?
* Qual a fun√ß√£o de ativa√ß√£o ?
*  Sabemos por padr√£o da pen√∫ltima  camada para tr√°s  usamos Relu e da camada final em diante e Softmax. 

Outras escolhas arquiteturais 

* Otimizador 
* Fun√ß√µes de Custo
* T√©cnicas de Normaliza√ß√£o 
* Dropout, Normalization, Noise

Depois de escolher voc√™ a Arquitetura Sequencial  

* Como as camadas estar√£o conectadas entre si ? 

Arquitetura Pararalela -> Percorre caminhos diferentes  e depois s√£o concatenadas

Framework Keras + TensorFlow   ( Deep Learning )

Deep Learning e muto caro computacionalmente 

* Paralelos ou divididos 
* Hardware GPU NVIDIA , TPU GOOGLE

Para o Framework Keras o Modelo √© Camadas e Otimizador 

keras.layers - Camadas

* Densa
* Convolucional  ( Conv2D )
* Pooling ( MaxPooling2D , AveragePooling2D )
* Regulariza√ß√£o ( Dropout, GaussianNose, BatchNormalization )

nn.layers - Camadas 

* Input
* Flatten
* Reshape
* Concatenate

![[Pasted image 20250325214851.png]]

![[Pasted image 20250325214905.png]]


![[Pasted image 20250325215139.png]]

![[Pasted image 20250325215245.png]]


Motiva√ß√µes de Usar Vis√£o Computacional no Tratamento de Feridas : 

1.  Automatiza√ß√£o de An√°lise de Feridas 
2. Precis√£o e Consist√™ncia 
3. An√°lise Multimodal
4. Capacidade de Lidar com Diversidade
5. Melhorias nos Tratamentos e Resultados
6. Redu√ß√£o de Custos 