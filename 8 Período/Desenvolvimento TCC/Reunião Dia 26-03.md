
Executar os Steps e Explicar [  ] 
Entender porque a dimens√£o 224x224 foi utilizada [ x ]  ( Poss√≠vel Testar em resolu√ß√£o 256x256 ou 512x512 )  
Procurar Artigos 5 √° 10 quais motiva√ß√µes de utilizar Vis√£o Computacional com Feridas. [ x ]  
### üìå **1. Padr√£o de Modelos Pr√©-Treinados**

A resolu√ß√£o **224√ó224** foi popularizada pelo modelo **AlexNet** (2012) e depois utilizada em redes como **VGG16, ResNet, MobileNet e EfficientNet**. Muitas arquiteturas modernas usam esse tamanho como entrada padr√£o, pois os pesos pr√©-treinados esperam essa dimens√£o.

**Exemplo:** Modelos como `ResNet-50` e `MobileNetV2` aceitam 224x224 como entrada padr√£o.
 
### üìå **2. Efici√™ncia Computacional e Mem√≥ria**

Redes neurais convolucionais (CNNs) operam melhor quando a imagem tem **tamanho fixo**. Se as imagens fossem maiores, exigiriam mais mem√≥ria e processamento. Se fossem muito pequenas, perderiam detalhes importantes.

- **224x224** √© um **compromisso entre qualidade e efici√™ncia**.
    
- **Menores que isso**: perda de detalhes.
    
- **Maiores que isso**: aumento do custo computacional.
    

**Exemplo pr√°tico:**  
Uma imagem de **1024x1024** tem **1.048.576 pixels**. J√° uma de **224x224** tem **50.176 pixels**, sendo **20√ó menor**, reduzindo o tempo de treinamento.

### **üìå 3. Preserva√ß√£o de Informa√ß√µes Relevantes**

Ao redimensionar para **224x224**, **mantemos detalhes essenciais** da imagem sem comprometer a qualidade para redes neurais convolucionais (CNNs).  
Se usarmos um tamanho muito pequeno, a CNN pode perder informa√ß√µes importantes, especialmente para **segmenta√ß√£o e classifica√ß√£o de les√µes cut√¢neas**.

**Exemplo:**  
Uma les√£o pequena pode desaparecer se reduzirmos para **64x64**, mas em **224x224** ainda conseguimos ver bordas e texturas.

### üìå **4. Aplica√ß√£o no Reconhecimento de Les√µes Cut√¢neas**

Em aplica√ß√µes m√©dicas, como a **segmenta√ß√£o e classifica√ß√£o de feridas**, a dimens√£o **224x224** √© suficiente para capturar caracter√≠sticas como:

- **Textura da pele**
    
- **Bordas da les√£o**
    
- **Padr√µes de colora√ß√£o**
    
- **Detalhes pequenos, mas importantes**
    

Se aumentarmos muito a resolu√ß√£o, apenas aumentamos o custo de processamento **sem melhorar significativamente a precis√£o**.

### üìå **5. Consist√™ncia entre Amostras**

Usar **224x224** para todas as imagens padroniza os dados de entrada, tornando o treinamento mais eficiente e evitando que a CNN precise lidar com tamanhos variados.

Sem um tamanho fixo, precisar√≠amos de **camadas adaptativas** ou realizar **padding**, o que poderia introduzir distor√ß√µes.


### ‚ö° **1. Redes  Convolucionais Par√¢metros T√©cnicos**

https://www.youtube.com/watch?v=lb5ocJiDLgg&t=1268s - Redes Neurais Convolucionais

Estamos Tratando de Dados Complexos ( Imagens )

* Imagem , Texto, Grafos, A√∫dio.

Dados Complexos : Alta dimensionalidade (Grande n√∫mero de Atributos e Caracter√≠sticas ) em outras palavras cada pixel e um dado.  Uma imagem de 7x7 pixels e menos que um √≠cone ou seja mesmo pequenas imagem pequenas existe uma quantidade absurda de atributos. ainda assim os dados s√£o " N√ÉO ESTRUTURADOS " pois as informa√ß√µes necess√°rias est√£o distribu√≠das em v√°rios pixels, dando o fato que precisa ser LIMPADO para conseguirmos extrair as caracter√≠sticas necess√°rias.

Problemas dos dados  Complexos : Redund√¢ncia doa dados, padr√µes distribu√≠dos em muitos atributos .  Feature Engineering  ( Limpeza de Dados, Transforma√ß√£o dos Dados, Sele√ß√£o das Caracter√≠sticas ), Reconhecimento de Padr√µes . 

Exemplo  : Conjunto de imagem de D√≠gitos  cujo objetivo e identificar dentro das imagens os subconjuntos de pixels (atributos caracter√≠sticas ) que identifique um determinado padr√£o  ent√£o o algoritmo deve encontrar as caracter√≠sticas mais fortes de uma classe para outras e ap√≥s a extra√ß√£o desse conjunto de pixels s√£o as caracter√≠sticas e ap√≥s extra√≠dos 

Aprendizagem de M√°quina Cl√°ssico ( Shallow ) era utilizado essa etapas at√© agora feito a m√£o

* Atributos Simples, Estruturados, Baixa Dimensionalidade  
*  Previamente Selecionados / Manipulados ( Engenharia de Caracter√≠sticas )
* Modelos Simples ( Rasos - Shallow ( OpenCV2  >> SVM  ( Classificador Linear Filtros  )))
* Poucos par√¢metros

Aprendizado Profundo ( Deep Learning ) em outra palavras o modelo aprende sozinho apenas com a imagem crua , sem que eu tenha que pr√©-processar os dados.

* Atributos Complexos :  N√£o estruturados, Alta Dimensionalidade
* Previamente Selecionados / Manipulados ( Engenharia de Caracter√≠sticas )
*  O pr√≥prio modelo aprende como : Selecionar e extrair caracter√≠sticas , reconhece padr√µes de caracter√≠sticas
* Modelos Complexos : Milhares  / Milh√µes de Par√¢metros 

Redes Neurais Convolucionais :  S√£o redes neurais que trabalham com qualquer tipo de dado por√©m vamos fixar com Imagens .

* Convolu√ß√µes de Dimens√£o 1 :  Para vetores
* Convolu√ß√µes de Dimens√£o 2 : Matrizes =  Imagens
* Convolu√ß√µes de Dimens√£o 3 : Matrizes de Matrizes = Volumes

Em outras palavras n√£o estamos criando uma outra rede neural , apenas estamos adicionando camadas novas dando suas respectivas responsabilidades.

*  Bloco 1  : Redu√ß√£o de Dimensionalidade , Extra√ß√£o de Caracter√≠sticas  ( Camadas de Convolu√ß√µes , por exemplo Pooling, Conv, ReLU )
*  Bloco 2 :  Reconhecimento de Classifica√ß√£o de Padr√µes

Tipos de Camadas : 

* Convolucionais
* Dimensionalidade (Pooling / Up Sampling )
* Densas / Totalmente Conectadas (MLP)
* Normaliza√ß√£o (Dropout , Normalization, Nose )

Fun√ß√µes de Ativa√ß√£o ( Para √∫ltima camada da Rede ) :

*  Fam√≠lia ReLU 
*  Fam√≠lia Softmax ( Cara Computacionalmente ) : Quanto mais sa√≠das eu tiver mais caro ser√° ( O(n ))

* Fun√ß√µes de Custo 
 
	Em outras palavras as CNN's elas ter√£o muitas camadas de convul√ß√£o, pooling, relu e perceptrons, com isso temos a primeira parte ou seja o Pass Forward e geralmente o output e a Classifica√ß√£o ou seja  essa sa√≠da e uma distribui√ß√£o de probabilidade que ser√° associada a cada classe poss√≠vel . O modelo ent√£o e treinado comparando a distribui√ß√£o de probabilidades com a classe verdadeira , sendo realizada pela fun√ß√£o de custo ( otimizador ) .


Da pen√∫ltima camada para tr√°s a gente usa a ReLU  seria o estado da arte √© que funciona bem para todos os modelos, e a √∫ltima camada e utilizado o Softmax ( problemas de classifica√ß√£o ) .


Otimizadores / M√©todos de Treinamento : 


Gradiente Descendente [ Batch / Mini-Batch / Stochastic ]  ( Pelo menos e oque estamos utilizando) e um Suavizador  dos pesos do modelo, 